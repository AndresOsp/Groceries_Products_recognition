{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test notebook\n",
    "\n",
    "## Grocery products recognition\n",
    "\n",
    "Hello and welcome to this notebook. \n",
    "\n",
    "This notebook trains a YOLOv2 algorithm in order to localize the grocery products in the supermarket shell.\n",
    "\n",
    "The following dataset is used to train this CNN:\n",
    "[Grocery Store dataset](https://www.amazon.de/clouddrive/share/J3OaZMNnhBpKG28mAfs5CqTgreQxFCY8uENGaIk7H3s?_encoding=UTF8&mgh=1&ref_=cd_ph_share_link_copy).\n",
    "\n",
    "This dataset was annotated in the work by [George, Marian and Floerkemeier](http://vision.disi.unibo.it/index.php?option=com_content&view=article&id=111&catid=78). (There is only 76 images annotated.)\n",
    "\n",
    "The Yolo (You Only Look Once) algorithm is presented in the following papers:\n",
    "* Redmon et al., 2016 (https://arxiv.org/abs/1506.02640) \n",
    "* Redmon and Farhadi, 2016 (https://arxiv.org/abs/1612.08242).\n",
    "\n",
    "The yolo algorithm was originally tested in [Darknet]( https://pjreddie.com/darknet). This algorithm has been implemented in python for several machine-learning frameworks. This work is based in the [YAD2K]( https://github.com/allanzelener/YAD2K) implementation for Keras and Tensorflow.\n",
    "\n",
    "#### This notebook will guide you in order to test your own network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import colorsys\n",
    "import imghdr\n",
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "from train_yolo_utils import draw\n",
    "\n",
    "from yad2k.models.keras_yolo import yolo_head, yolo_boxes_to_corners, preprocess_true_boxes, yolo_loss, yolo_body, yolo_eval, tiny_yolo_body\n",
    "from yad2k.utils.draw_boxes import draw_boxes\n",
    "\n",
    "\n",
    "from yolo_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because some problems with windowns 10, the test images are computed with the CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "config = tf.ConfigProto(device_count = {'GPU': 0})\n",
    "set_session(tf.Session(config=config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Paths of anchors and classes\n",
    "anchors_path = 'model_data/yolo_anchors.txt'\n",
    "classes_path = 'model_data/groceries_classes.txt'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Please put the images to test in a folder and add the path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Path where the test images are located\n",
    "test_path    = 'images'\n",
    "# Path where the test images will be located after the YOLO algorithm\n",
    "output_path  = 'images/out'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Program Files\\Anaconda3\\lib\\site-packages\\keras\\models.py:251: UserWarning: No training configuration found in save file: the model was *not* compiled. Compile it manually.\n",
      "  warnings.warn('No training configuration found in save file: '\n"
     ]
    }
   ],
   "source": [
    "# load the model\n",
    "yolo_model = load_model('model_data/Products_recognition_body.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load the weights for the model\n",
    "yolo_model.load_weights('model_data/trained_stage_3_best.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read classes and anchors\n",
    "class_names = read_classes(classes_path)\n",
    "anchors = read_anchors(anchors_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['images/16.jpg', 'images/253.jpg', 'images/46.jpg', 'images/54.jpg', 'images/55.jpg']\n"
     ]
    }
   ],
   "source": [
    "# get images paths\n",
    "images_path=[]\n",
    "for image_file in os.listdir(test_path):\n",
    "    try:\n",
    "        image_type = imghdr.what(os.path.join(test_path, image_file))\n",
    "        if not image_type:\n",
    "            continue\n",
    "    except:\n",
    "        continue\n",
    "        \n",
    "    images_path.append(test_path+'/'+image_file)\n",
    "\n",
    "print(images_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# preprocess the images to the format of the yolo algorithm\n",
    "processed_images, images = preprocess_images(images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5, 1, 416, 416, 3)\n",
      "Found 14 boxes for image.\n",
      "[[  36.3269577   132.84387207  128.59158325  194.58450317]\n",
      " [ 227.73295593   76.09931946  312.1960144   137.14862061]\n",
      " [  43.55762482  199.31077576  131.93045044  261.74081421]\n",
      " [ 144.96020508  327.07556152  211.02326965  386.02005005]\n",
      " [  40.95552063  338.77905273  135.22770691  380.08422852]\n",
      " [ 233.10205078  169.49177551  315.68237305  238.97850037]\n",
      " [ 242.76408386  303.32897949  321.63592529  368.9102478 ]\n",
      " [ 320.47808838  171.26382446  390.99334717  250.06095886]\n",
      " [ 236.08024597  245.36761475  321.62112427  298.12075806]\n",
      " [ -14.97159767   -3.56864429  208.97059631   81.2219696 ]\n",
      " [  32.50404358   78.65345764  123.47944641  133.57655334]\n",
      " [ 202.59315491    8.41079903  339.61422729   65.44841766]\n",
      " [ 326.88937378   74.22484589  391.64422607  166.14689636]\n",
      " [ 323.07443237  272.48623657  398.87393188  337.66030884]]\n",
      "(5, 1, 416, 416, 3)\n",
      "product 0.70 (133, 36) (195, 129)\n",
      "product 0.70 (76, 228) (137, 312)\n",
      "product 0.67 (199, 44) (262, 132)\n",
      "product 0.60 (327, 145) (386, 211)\n",
      "product 0.50 (339, 41) (380, 135)\n",
      "product 0.49 (169, 233) (239, 316)\n",
      "product 0.43 (303, 243) (369, 322)\n",
      "product 0.33 (171, 320) (250, 391)\n",
      "product 0.30 (245, 236) (298, 322)\n",
      "product 0.23 (0, 0) (81, 209)\n",
      "product 0.23 (79, 33) (134, 123)\n",
      "product 0.18 (8, 203) (65, 340)\n",
      "product 0.04 (74, 327) (166, 392)\n",
      "product 0.04 (272, 323) (338, 399)\n",
      "Found 10 boxes for image.\n",
      "[[ 361.99865723  222.78294373  426.98852539  392.12988281]\n",
      " [ 267.64556885  223.11079407  331.13360596  389.21270752]\n",
      " [   3.02889514  233.19920349   67.20966339  395.31164551]\n",
      " [ 175.54910278   52.60538483  242.72331238  222.33325195]\n",
      " [  87.36392212   49.85732651  154.88032532  214.42735291]\n",
      " [  -4.75347996   53.21261215   63.72720718  210.76052856]\n",
      " [ 266.24035645   42.86885452  334.61447144  210.11697388]\n",
      " [  91.26016998  223.79078674  160.70062256  394.27285767]\n",
      " [ 360.93771362   38.89825821  425.87860107  205.28871155]\n",
      " [ 181.31613159  236.661026    244.33651733  397.54318237]]\n",
      "(5, 1, 416, 416, 3)\n",
      "product 1.00 (223, 362) (392, 416)\n",
      "product 0.99 (223, 268) (389, 331)\n",
      "product 0.99 (233, 3) (395, 67)\n",
      "product 0.99 (53, 176) (222, 243)\n",
      "product 0.99 (50, 87) (214, 155)\n",
      "product 0.99 (53, 0) (211, 64)\n",
      "product 0.99 (43, 266) (210, 335)\n",
      "product 0.99 (224, 91) (394, 161)\n",
      "product 0.99 (39, 361) (205, 416)\n",
      "product 0.99 (237, 181) (398, 244)\n",
      "Found 13 boxes for image.\n",
      "[[ 111.66201782   55.9546814   237.66593933  120.29193115]\n",
      " [ 129.4879303   -17.8216362   244.28315735   41.3311882 ]\n",
      " [ 270.43222046  194.36889648  373.44778442  247.72839355]\n",
      " [ 272.3260498   331.66088867  397.38360596  384.25961304]\n",
      " [ 112.88570404  334.58374023  248.2141571   391.87591553]\n",
      " [ 286.36721802  258.82583618  397.00482178  314.90151978]\n",
      " [ 108.51534271  248.36306763  251.17167664  304.04391479]\n",
      " [ 273.75418091   79.63211823  360.6789856   130.37236023]\n",
      " [ 108.29629517  182.5684967   245.30212402  244.10061646]\n",
      " [ 289.98214722   -1.46992719  395.02993774   45.14323044]\n",
      " [ 281.55200195  128.59527588  397.54455566  189.70336914]\n",
      " [ 101.46016693  117.53379822  245.96897888  179.0436554 ]\n",
      " [ 275.04974365  388.8420105   400.17654419  436.99630737]]\n",
      "(5, 1, 416, 416, 3)\n",
      "product 1.00 (56, 112) (120, 238)\n",
      "product 1.00 (0, 129) (41, 244)\n",
      "product 1.00 (194, 270) (248, 373)\n",
      "product 1.00 (332, 272) (384, 397)\n",
      "product 1.00 (335, 113) (392, 248)\n",
      "product 1.00 (259, 286) (315, 397)\n",
      "product 1.00 (248, 109) (304, 251)\n",
      "product 1.00 (80, 274) (130, 361)\n",
      "product 1.00 (183, 108) (244, 245)\n",
      "product 1.00 (0, 290) (45, 395)\n",
      "product 0.99 (129, 282) (190, 398)\n",
      "product 0.99 (118, 101) (179, 246)\n",
      "product 0.99 (389, 275) (416, 400)\n",
      "Found 10 boxes for image.\n",
      "[[ 181.20170593  339.25817871  283.41351318  399.20141602]\n",
      " [ 185.71116638  281.64376831  279.6786499   335.18569946]\n",
      " [ 285.22351074  159.09727478  416.51544189  249.43099976]\n",
      " [ 294.43533325  347.75158691  404.65930176  450.38693237]\n",
      " [ 188.2562561   182.33546448  281.87219238  239.79994202]\n",
      " [ 288.24423218   44.92912674  412.5057373   123.15040588]\n",
      " [ 188.6933136    26.95606041  265.47125244   86.38431549]\n",
      " [ 302.72601318  -19.02590561  404.81954956   41.05540848]\n",
      " [ 186.74859619  121.79327393  270.44198608  172.09915161]\n",
      " [ 294.92077637  244.9254303   406.85873413  342.63113403]]\n",
      "(5, 1, 416, 416, 3)\n",
      "product 0.98 (339, 181) (399, 283)\n",
      "product 0.96 (282, 186) (335, 280)\n",
      "product 0.57 (159, 285) (249, 416)\n",
      "product 0.57 (348, 294) (416, 405)\n",
      "product 0.51 (182, 188) (240, 282)\n",
      "product 0.47 (45, 288) (123, 413)\n",
      "product 0.17 (27, 189) (86, 265)\n",
      "product 0.12 (0, 303) (41, 405)\n",
      "product 0.11 (122, 187) (172, 270)\n",
      "product 0.09 (245, 295) (343, 407)\n",
      "Found 10 boxes for image.\n",
      "[[ 246.02856445  307.46087646  410.82580566  374.81591797]\n",
      " [ 253.43449402  241.1987915   404.06362915  304.44128418]\n",
      " [ 244.17605591   66.54732513  438.15301514  162.88619995]\n",
      " [ 106.99291992  273.32879639  189.07171631  351.19042969]\n",
      " [ 134.65432739   65.48312378  203.2311554   177.68217468]\n",
      " [ -38.43463516  253.18199158   66.08289337  354.22280884]\n",
      " [  93.32486725   -7.55379295  187.81590271   36.15895844]\n",
      " [ 259.34143066  147.45109558  340.56628418  217.7412262 ]\n",
      " [ 268.84997559  -27.20137215  400.5982666    51.5265274 ]\n",
      " [  99.38800049  369.88470459  184.56382751  421.2371521 ]]\n",
      "(5, 1, 416, 416, 3)\n",
      "product 0.98 (307, 246) (375, 411)\n",
      "product 0.97 (241, 253) (304, 404)\n",
      "product 0.96 (67, 244) (163, 416)\n",
      "product 0.59 (273, 107) (351, 189)\n",
      "product 0.31 (65, 135) (178, 203)\n",
      "product 0.16 (253, 0) (354, 66)\n",
      "product 0.13 (0, 93) (36, 188)\n",
      "product 0.11 (147, 259) (218, 341)\n",
      "product 0.08 (0, 269) (52, 401)\n",
      "product 0.06 (370, 99) (416, 185)\n"
     ]
    }
   ],
   "source": [
    "# Test the images\n",
    "draw(yolo_model,\n",
    "    class_names,\n",
    "    anchors,\n",
    "    processed_images,\n",
    "    image_set='all', # assumes training/validation split is 0.9\n",
    "    weights_name='model_data/trained_stage_3_best.h5',\n",
    "    out_path=\"images/out\",\n",
    "    save_all=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To see this images go to \"images/out\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
